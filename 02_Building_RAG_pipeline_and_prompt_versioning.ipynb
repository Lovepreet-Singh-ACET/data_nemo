{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environemnt Variables\n",
    "\n",
    "Create a `.env` file in your project directory and save the following.\n",
    "\n",
    "```\n",
    "PINECONE_API_KEY = \"<your api key>\"\n",
    "OPENAI_API_KEY = \"<your api key>\"\n",
    "LANGCHAIN_API_KEY = \"<your api key>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment variable\n",
    "\n",
    "`python-dotenv` package can be used as shown below to load the `.env` file we just created and then using `os` module we can set the environemnt variables.\n",
    "\n",
    "To install: `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings # To create embeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore # To connect with the Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining globle variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'earning-calls'\n",
    "TOP_K = 6\n",
    "QUARTER = \"Q1\"\n",
    "FILENAME = \"Adani Enterprises Ltd.pdf\"\n",
    "YEAR = \"FY24\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings) # loading the index\n",
    "retriver = index.as_retriever(search_kwargs={\"filter\": {\"quarter\": QUARTER, \"filename\": FILENAME, \"year\": YEAR}, \"k\": TOP_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='mining is, all our commercial mining activities, be it India or overseas will fall under the \\ncommercial mining which fall under our natural resource s region of which my colleague Vinay \\nis the CEO . \\nModerator:  Thank you. The  next question is from the line of Gaurav Singhal from Aspex Management \\nLimited . Please go ahead.  \\nGaurav Singhal:  Two questions from me. So, one is, can you help give a break up of the $3.7 billion CAPEX plan \\nfor this year  across the different segment s? Thank you.  \\nRobbie Singh : Approximately about $300 million for Green Hydrogen, $1.1 billi on for airports, approximately \\n$1.7 billion for the road network, just under $100 million  for water , just under $200 million for \\nthe Data Center, and then small completion cost for the copper project just under $200 million.  \\nGaurav Singhal:  And then secondly in terms of financing the CAPEX, the board had passed resolution to al low \\nthe group  to raise about 12,500 crores in equity, in that part of the financing for this CAPEX ?', metadata={'filename': 'Adani Enterprises Ltd.pdf', 'page': 8.0, 'quarter': 'Q1', 'source': '/home/love/Desktop/Projects/TestUploadData/FY24/Q1/Adani Enterprises Ltd.pdf', 'year': 'FY24'}),\n",
       " Document(page_content='acquire it. And therefore,  total CAPEX as we had indicated even last year , this year was around \\n$3.7 billion across Adani Enterprise and will continue that way.  Another on the utility portfolio , \\nwith the utility portfolio that we have, and transport and logistics portfolio we have and the core \\nasset portfolio we have which is in the primary industry. They are driven by the fundamentals \\nof the users and consumers in India plus the demands in India which  is not changing. So, a report', metadata={'filename': 'Adani Enterprises Ltd.pdf', 'page': 6.0, 'quarter': 'Q1', 'source': '/home/love/Desktop/Projects/TestUploadData/FY24/Q1/Adani Enterprises Ltd.pdf', 'year': 'FY24'}),\n",
       " Document(page_content='We would be well on the way to  completing 1 gigaw att before the end of th is decade . \\nAditya Bhartia : And lastly, sir what are our CAPEX plans especially for green hydrogen, airports and data center \\nsegments for the next 3 years?  \\nRobbie Singh : Overall in the longer term , the CAPEX plans don’t alter for the asset like for green hydrogen  \\nfull 3-million -ton facility , approximately $ 50 billion as we have outlined  in previous  year, so \\nthat plan continues forward as it is . Also this year,  we would touch just about between 300 \\nmillion to 400 million and then it rapidly starts rising from next year  and the year after . On the', metadata={'filename': 'Adani Enterprises Ltd.pdf', 'page': 4.0, 'quarter': 'Q1', 'source': '/home/love/Desktop/Projects/TestUploadData/FY24/Q1/Adani Enterprises Ltd.pdf', 'year': 'FY24'}),\n",
       " Document(page_content='are seeing is basically the core CAPEX, be it Adani Green, be it Adani Transmission, be it Adani \\nPorts, be it Adani Total Gas, be it AEL, be it Airports, be it the Green Hydrogen Ecosystem, be \\nit Datacenter, everything is continuing a s normal. So, it was only the perception outside that \\nsomething is going to be a slowdown in the core, that w as never the case. We never said that.  \\nAnd what you are seeing here is naturally when the opportunity came along, it was a key \\nopportunity for our  portfolio Company , Ambuja to acquire a great asset available for which is \\nsynergistic to its portfolio, synergistic from a logistics point of view, massively positive in terms \\nof earnings. They did that. If any asset like this comes available to us in a core portfolio , we will \\nacquire it. And therefore,  total CAPEX as we had indicated even last year , this year was around \\n$3.7 billion across Adani Enterprise and will continue that way.  Another on the utility portfolio ,', metadata={'filename': 'Adani Enterprises Ltd.pdf', 'page': 6.0, 'quarter': 'Q1', 'source': '/home/love/Desktop/Projects/TestUploadData/FY24/Q1/Adani Enterprises Ltd.pdf', 'year': 'FY24'}),\n",
       " Document(page_content='the overall CAPEX for FY24 for the Company  and I mean even we have completed a large \\nM&A in our group Company  cement business,  and this probably is the first large obviously \\nM&A  after the Hindenburg  report . Are you looking for accelerating CAPEX in Adani Enterprise \\nas well which was just said to have sort of mild slow down in past quarter?  \\nRobbie Singh : No, I think that was probably more media than anything else. We had always said around 28th, \\n29th of January . We had said that in the core businesses the CAPEX will continue. There is \\nabsolutely no reason for any of that to ha ppen. That whole frenzy of half -baked articles and \\nstories were related to people’s perception rather than what we had actually said. So, what you \\nare seeing is basically the core CAPEX, be it Adani Green, be it Adani Transmission, be it Adani \\nPorts, be it Adani Total Gas, be it AEL, be it Airports, be it the Green Hydrogen Ecosystem, be', metadata={'filename': 'Adani Enterprises Ltd.pdf', 'page': 6.0, 'quarter': 'Q1', 'source': '/home/love/Desktop/Projects/TestUploadData/FY24/Q1/Adani Enterprises Ltd.pdf', 'year': 'FY24'}),\n",
       " Document(page_content='rate will rise,  margins will not rise, but are expected to stabilize or very slight decline . But we \\nexpect that and the reason for the high margin is that technology plus the trade flows mean that \\nthere is a demand for product from India and that is what is pushing the margins higher due to \\nthe competitive pressures in relation to South East Asia, the ir cost requirement s, their sale costs \\netc. So, it is a n overall global supply -based  scenario. We expect that to continue for some time, \\nbut over the longer term , we expect the volumes to be high, margins to be slightly tighter than \\nwhere they are today.  \\nModerator:  Thank you. The next question is from the line of Prateek Kumar from Jefferies. P lease go ahead.  \\nPrateek Kumar:  My first question is on your CAPEX, as y ou highlighted CAPEX segment wise,  so what will be \\nthe overall CAPEX for FY24 for the Company  and I mean even we have completed a large \\nM&A in our group Company  cement business,  and this probably is the first large obviously', metadata={'filename': 'Adani Enterprises Ltd.pdf', 'page': 6.0, 'quarter': 'Q1', 'source': '/home/love/Desktop/Projects/TestUploadData/FY24/Q1/Adani Enterprises Ltd.pdf', 'year': 'FY24'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke(\"what is the capax?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"),\n",
    "        (\"human\", \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query}\\nAnswer: \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an expert Q&A system that is trusted around the world.\n",
      "Always answer the query using the provided context information, and not prior knowledge.\n",
      "Some rules to follow:\n",
      "1. Never directly reference the given context in your answer.\n",
      "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
      "Human: Context information is below.\n",
      "---------------------\n",
      "THIS IS A SAMPLE CONTEXT TO SEE HOW THE PROMPT LOOKS\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: THIS IS A SAMPLE QUERY?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(chat_template.format(context=\"THIS IS A SAMPLE CONTEXT TO SEE HOW THE PROMPT LOOKS\", query=\"THIS IS A SAMPLE QUERY?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | chat_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriver, \"query\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The consolidated total income was at Rs. 25,810 crores.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain_with_source.invoke(\"What was the income?\")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Versioning\n",
    "\n",
    "Loading a Specific Version of a Prompt:\n",
    "\n",
    "1. **Version Tracking in Repositories:**\n",
    "   - Each push to a prompt repository saves a new version, identified by a unique commit hash.\n",
    "\n",
    "2. **Loading the Latest Version:**\n",
    "   - By default, accessing the repo will load the most recent version of a given prompt.\n",
    "\n",
    "3. **Loading a Specific Version:**\n",
    "   - To load a specific version, include its commit hash with the prompt name.\n",
    "   - Example: For loading the \"earning-call-rag\" with version `6214c98a`, append this hash to the prompt name in your loading command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"lovepreet/earning-call-rag:6214c98a\") # A prompt can be created either locally and pushed to hub or it can be created directly on the hub and then pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an expert Q&A system that is trusted around the world.\n",
      "Always answer the query using the provided context information, and not prior knowledge.\n",
      "Some rules to follow:\n",
      "1. Never directly reference the given context in your answer.\n",
      "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
      "Human: Context information is below.\n",
      "---------------------\n",
      "THIS IS A SAMPLE CONTEXT TO SEE HOW THE PROMPT LOOKS\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: THIS IS A SAMPLE QUERY?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(context=\"THIS IS A SAMPLE CONTEXT TO SEE HOW THE PROMPT LOOKS\", query=\"THIS IS A SAMPLE QUERY?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt # placing the newly loaded prompt here\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriver, \"query\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The consolidated total income was at Rs. 25,810 crores.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain_with_source.invoke(\"What was the income?\")\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Share Prompts on LangChain Hub:\n",
    "\n",
    "1. **Getting Started:**\n",
    "   - Getting prompts from LangChain Hub is easy, and so is sharing your own prompts.\n",
    "\n",
    "   - This lets you easily share and manage your own prompts.\n",
    "\n",
    "2. **Making a Prompt:**\n",
    "\n",
    "   - First, create a prompt that fits what you need.\n",
    "\n",
    "   - Make sure it follows the rules of LangChain Hub.\n",
    "\n",
    "3. **Sharing Process:**\n",
    "\n",
    "   - The sharing has two important parts:\n",
    "     - **Account Handle:** Your special name in LangChain Hub, like `me-langchain-user`.\n",
    "     - **Prompt Name:** A clear name for your prompt, showing what it does.\n",
    "\n",
    "4. **How to Share with Code:**\n",
    "   - This is a simple way to share:\n",
    "     ```python\n",
    "     from langchain import hub\n",
    "\n",
    "     # Define your prompt\n",
    "     my_prompt = \"...\"  # Your prompt content goes here\n",
    "\n",
    "     # Share it on LangChain Hub\n",
    "     hub.push(f\"{account_handle}/{prompt_name}\", my_prompt)\n",
    "     ```\n",
    "     - Replace `account_handle` with your username and `my_prompt` with your prompt's name.\n",
    "     \n",
    "     - Make sure `my_prompt` follows LangChain PromptTemplate.\n",
    "\n",
    "5. **Using Your Shared Prompt:**\n",
    "   - Once it's shared, you can use it in different apps through LangChain Hub.\n",
    "   - This makes it easy to share with others, work together, and manage your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\")),\n",
       "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], template='Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query}\\nAnswer: '))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.messages), prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], template='Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query}\\nAnswer: '))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'query'], template='Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query}\\nAnswer: ')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[1].prompt.template = 'Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nUse Bullet poits whenever possible in the answer.\\nQuery: {query}\\nAnswer: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Use Bullet poits whenever possible in the answer.\n",
      "Query: {query}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages[1].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added the following line in the prompt: `Use Bullet poits whenever possible in the answer.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = os.getenv(\"LANGSMIT_USER_HANDLE\")\n",
    "\n",
    "prompt_url = hub.push(f\"{handle}/earning-call-rag\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/hub/lovepreet/earning-call-rag/359db5ba'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
