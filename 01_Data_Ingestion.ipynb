{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "In this tutorial we show how create a data ingenstion pipeline to add data to a vector database.\n",
    "\n",
    "We are going to use `Pinecone` as the vector database, but there are other vector databases available too for example `Chroma, Weaviate, Faiss, etc.`\n",
    "\n",
    "We will be doing the following in this session:\n",
    "- How to load in documents.\n",
    "- Add metadata to each document.\n",
    "- How to use a text splitter to split documents.\n",
    "- How to generate embeddings for each text chunk.\n",
    "- How to insert into a vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone\n",
    "\n",
    "You will need a [Pinecone](https://www.pinecone.io/) API key, you can [sign-up](https://app.pinecone.io/?sessionType=signup) for free to get a started account and then get the API key after sign-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "You will need an [OpenAI](https://openai.com/) api key for this session. Login to your [platform.openai.com](https://platform.openai.com/) account, click on your profile picture in the upper right corner, and choose 'API Keys' from the menu. Create an API key and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environemnt Variables\n",
    "\n",
    "Create a `.env` file in your project directory and save the following.\n",
    "\n",
    "```\n",
    "PINECONE_API_KEY = \"<your api key>\"\n",
    "OPENAI_API_KEY = \"<your api key>\"\n",
    "LANGCHAIN_API_KEY = \"<your api key>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment variable\n",
    "\n",
    "`python-dotenv` package can be used as shown below to load the `.env` file we just created and then using `os` module we can set the environemnt variables.\n",
    "\n",
    "To install: `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index `earning-calls-euclidean` already exists\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 3039}},\n",
      " 'total_vector_count': 3039}\n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "INDEX_NAME = 'earning-calls-euclidean'\n",
    "USE_SERVERLESS = True\n",
    "\n",
    "# configure client  \n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])  \n",
    "\n",
    "if USE_SERVERLESS:  \n",
    "    spec = ServerlessSpec(cloud='aws', region='us-east-1')   \n",
    "    # check if already exists\n",
    "    if INDEX_NAME in pc.list_indexes().names():\n",
    "        print(f\"Index `{INDEX_NAME}` already exists\")\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        print(index.describe_index_stats())\n",
    "    # create a new index \n",
    "    else: \n",
    "        pass       \n",
    "        # pc.create_index(\n",
    "        #     INDEX_NAME,\n",
    "        #     dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        #     metric='cosine',\n",
    "        #     spec=spec\n",
    "        # )\n",
    "        # # wait for index to be initialized\n",
    "        # while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "        #     time.sleep(1)\n",
    "        # print(f\"Index with name `{INDEX_NAME}` is created\")\n",
    "        # index = pc.Index(INDEX_NAME)\n",
    "        # print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` In case you want to delete an already existing index then use the following `pc.delete_index(index_name)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # To split the text into smaller chunks\n",
    "from langchain_openai import OpenAIEmbeddings # To create embeddings\n",
    "from langchain_pinecone import PineconeVectorStore # To connect with the Vectorstore\n",
    "from langchain_community.document_loaders import DirectoryLoader # To load files in a directory\n",
    "from langchain_community.document_loaders import PyPDFLoader # To parse the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"DATA_2/\" # Path to the Data directory\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 204\n",
    "INDEX_NAME = 'earning-calls-euclidean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` Make sure to maintain the below show directory structure since we will be using the Year and Quarter directory names in the metadata later.\n",
    "\n",
    "<!-- ![Data Dir Tree](images/data_dir_tree.png) -->\n",
    "\n",
    "<img src=\"images/data_dir_tree.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files\n",
    "\n",
    "Initialize a DirectoryLoader object and pass the `Path to data`, `the type of files to load from directory`, and `the loader_class` which in our case is PyPDFLoader since we are working with PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents loaded: 881\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(path=DATA_DIR_PATH, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Total Documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert the Document object to a python dict using the .dict() method.\n",
    "print(f\"keys associated with a Document: {docs[0].dict().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'-'*15}\\nFirst 100 charachters of the page content: {docs[0].page_content[:100]}\\n{'-'*15}\")\n",
    "print(f\"Metadata associated with the document: {docs[0].metadata}\\n{'-'*15}\")\n",
    "print(f\"Datatype of the document: {docs[0].type}\\n{'-'*15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We loop through each document and add additional metadata - filename, quarter, and year\n",
    "for doc in docs:\n",
    "    filename = doc.dict()['metadata']['source'].split(\"/\")[-1]\n",
    "    quarter = doc.dict()['metadata']['source'].split(\"/\")[-2]\n",
    "    year = doc.dict()['metadata']['source'].split(\"/\")[-3]\n",
    "    doc.metadata = {\"filename\": filename, \"quarter\": quarter, \"year\": year, \"source\": doc.dict()['metadata']['source'], \"page\": doc.dict()['metadata']['page']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To veryfy that the metadata is indeed added to the document\n",
    "print(f\"Metadata associated with the document: {docs[0].metadata}\\n{'-'*15}\")\n",
    "print(f\"Metadata associated with the document: {docs[1].metadata}\\n{'-'*15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Text\n",
    "\n",
    "As the name suggests, chunking is the process of dividing a large amount of data into several smaller parts for more effective and meaningful storage.\n",
    "\n",
    "There are various ways to perform chunking naming some as:\n",
    " - Character Chunking\n",
    " - Recursive Character Chunking\n",
    " - Document Specific Chunking\n",
    "\n",
    "For the sake of this session we will be using the `Recursive Character Chunking` and langchain has an implemention that we can directly use. To read more about it you can refer to the [docs](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/)\n",
    "\n",
    "`Additional Resource:` If you want to explore the different chunking stratigies than you can refer to the following docs from langchain - [Link to Docs](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "     chunk_size=CHUNK_SIZE,\n",
    "     chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\") # Initialize the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_already_in_pinecone = input(\"Are the vectors already added in DB: (Type Y/N)\")\n",
    "\n",
    "# check if the documents were already added to the vector database\n",
    "if docs_already_in_pinecone == \"Y\" or docs_already_in_pinecone == \"y\":\n",
    "    docsearch = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "    print(\"Existing Vectorstore is loaded\")\n",
    "# if not then add the documents to the vectore db\n",
    "elif docs_already_in_pinecone == \"N\" or docs_already_in_pinecone == \"n\":\n",
    "    docsearch = PineconeVectorStore.from_documents(documents, embeddings, index_name=INDEX_NAME)\n",
    "    print(\"New vectorstore is created and loaded\")\n",
    "else:\n",
    "    print(\"Please type Y - for yes and N - for no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are defing how to use the loaded vectorstore as retriver\n",
    "retriver = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver.invoke(\"what is the income?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using metadata with retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = docsearch.as_retriever(search_kwargs={\"filter\": {\"quarter\": \"Q1\"}, \"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver.invoke(\"what is the income?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
